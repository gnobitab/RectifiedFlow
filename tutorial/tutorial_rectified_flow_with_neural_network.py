# -*- coding: utf-8 -*-
"""Tutorial: Rectified Flow with Neural Network.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CyUP5xbA3pjH55HDWOA8vRgk2EEyEl_P

# Rectified Flow
This jupyter notebook contains simple tutorial code for Rectified Flow proposed in '[Flow Straight and Fast: Learning
to Generate and Transfer Data with Rectified Flow](https://arxiv.org/abs/2209.03003)'.

The problem here is to learn an ODE $\dot Z_t = v(Z_t, t) $ to transfer data from $\pi_0$ to $\pi_1$, where both
$\pi_0$ and $\pi_1$ are unknown and empirically observed through a set of points.

The velocity field $v(z,t)$ in rectified flow can be fitted with either kernel method or deep neural networks.
This tutorial illustrates the use of a neural network.

## Generating Distribution $\pi_0$ and $\pi_1$
We generate $\pi_0$ and $\pi_1$ as two Gaussian mixture models with different modes.

We sample 10000 data points from $\pi_0$ and $\pi_1$, respectively,
and store them in ```samples_0```, ```samples_1```.
"""

import torch
import numpy as np
import torch.nn as nn
from sklearn.datasets import make_swiss_roll, make_circles, make_blobs
from torch.distributions import Normal, Categorical
from torch.distributions.multivariate_normal import MultivariateNormal
from torch.distributions.mixture_same_family import MixtureSameFamily
import matplotlib.pyplot as plt
from tqdm import tqdm


@torch.no_grad()
def draw_plot(rectified_flow, z0, z1, N=None):
    traj = rectified_flow.sample_ode(z0=z0, N=N)
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)
    fig.suptitle('Actual vs Generated Distribution')
    x_lim = (-10, 10)
    y_lim = (-10, 10)

    ax1.set_ylim(y_lim)
    ax1.set_xlim(x_lim)
    ax1.set_title(r'$\pi_0$')
    ax1.scatter(traj[0][:, 0].cpu().numpy(), traj[0][:, 1].cpu().numpy(), color='red')

    ax2.set_ylim(y_lim)
    ax2.set_xlim(x_lim)
    ax2.set_title(r'$\pi_1$')
    ax2.scatter(z1[:, 0].cpu().numpy(), z1[:, 1].cpu().numpy(), color='green')

    ax3.set_ylim(y_lim)
    ax3.set_xlim(x_lim)
    ax3.set_title('Generated')
    ax3.scatter(traj[-1][:, 0].cpu().numpy(), traj[-1][:, 1].cpu().numpy(), color='blue')
    plt.tight_layout()
    plt.savefig("actual_vs_generated_samples.png")

    plt.clf()

    # Plot trajectories
    traj_particles = torch.stack(traj)
    plt.figure(figsize=(4, 4))
    plt.xlim(-M, M)
    plt.ylim(-M, M)
    plt.axis('equal')
    for i in range(30):
        plt.plot(traj_particles[:, i, 0], traj_particles[:, i, 1])
    plt.title('Transport Trajectory')
    plt.tight_layout()
    plt.savefig("trajectory.png")


def train_rectified_flow(rectified_flow, optimizer, pairs, batchsize, inner_iters):
    loss_curve = []
    for i in tqdm(range(inner_iters + 1), desc="training"):
        optimizer.zero_grad()
        indices = torch.randperm(len(pairs))[:batchsize]
        batch = pairs[indices]
        z0 = batch[:, 0].detach().clone()
        z1 = batch[:, 1].detach().clone()
        z_t, t, target = rectified_flow.get_train_tuple(z0=z0, z1=z1)

        pred = rectified_flow.model(z_t, t)
        loss = (target - pred).view(pred.shape[0], -1).abs().pow(2).sum(dim=1)
        loss = loss.mean()
        loss.backward()

        optimizer.step()
        loss_curve.append(np.log(loss.item()))  ## to store the loss curve

    return rectified_flow, loss_curve


class MLP(nn.Module):
    def __init__(self, input_dim=2, hidden_num=100):
        super().__init__()
        self.fc1 = nn.Linear(input_dim + 1, hidden_num, bias=True)
        self.fc2 = nn.Linear(hidden_num, hidden_num, bias=True)
        self.fc3 = nn.Linear(hidden_num, input_dim, bias=True)
        self.act = lambda x: torch.tanh(x)

    def forward(self, x_input, t):
        inputs = torch.cat([x_input, t], dim=1)
        x = self.fc1(inputs)
        x = self.act(x)
        x = self.fc2(x)
        x = self.act(x)
        x = self.fc3(x)

        return x


class RectifiedFlow():
    def __init__(self, model=None, num_steps=1000):
        self.model = model
        self.N = num_steps

    def get_train_tuple(self, z0=None, z1=None):
        t = torch.rand((z1.shape[0], 1))
        z_t = t * z1 + (1. - t) * z0
        target = z1 - z0

        return z_t, t, target

    @torch.no_grad()
    def sample_ode(self, z0=None, N=None):
        ### NOTE: Use Euler method to sample from the learned flow
        if N is None:
            N = self.N
        dt = 1. / N
        traj = []  # to store the trajectory
        z = z0.detach().clone()
        batchsize = z.shape[0]

        traj.append(z.detach().clone())
        for i in range(N):
            t = torch.ones((batchsize, 1)) * i / N
            pred = self.model(z, t)
            z = z.detach().clone() + pred * dt
            traj.append(z.detach().clone())
        return traj


if __name__ == '__main__':
    D = 10.
    M = D + 5
    VAR = 0.3
    DOT_SIZE = 4
    COMP = 3
    N = 10000
    # initial_mix = Categorical(torch.tensor([1 / COMP for i in range(COMP)]))
    # initial_comp = MultivariateNormal(torch.tensor(
    #     [[D * np.sqrt(3) / 2., D / 2.], [-D * np.sqrt(3) / 2., D / 2.], [0.0, - D * np.sqrt(3) / 2.]]).float(),
    #                                   VAR * torch.stack([torch.eye(2) for i in range(COMP)]))
    # initial_model = MixtureSameFamily(initial_mix, initial_comp)
    initial_model = MultivariateNormal(loc=torch.zeros(2), covariance_matrix=torch.eye(2))
    samples_0 = initial_model.sample(torch.Size([N]))

    # target_mix = Categorical(torch.tensor([1 / COMP for i in range(COMP)]))
    # target_comp = MultivariateNormal(torch.tensor(
    #     [[D * np.sqrt(3) / 2., - D / 2.], [-D * np.sqrt(3) / 2., - D / 2.], [0.0, D * np.sqrt(3) / 2.]]).float(),
    #                                  VAR * torch.stack([torch.eye(2) for i in range(COMP)]))
    # target_model = MixtureSameFamily(target_mix, target_comp)
    # samples_1 = target_model.sample(torch.Size([N]))
    # Swissroll
    samples_1 = torch.tensor(make_swiss_roll(n_samples=N, noise=1e-1)[0][:, [0, 2]] / 2.0, dtype=torch.float32)
    # Circles
    # samples_1 = torch.tensor(make_circles(n_samples=N, shuffle=True, factor=0.9, noise=0.05)[0] * 5.0,dtype=torch.float32)
    # Blobs
    # samples_1 = torch.tensor(make_blobs(n_samples=N, n_features=2, centers=[[-1, -1], [0, 0], [1, 1]])[0],
    #                          dtype=torch.float32)
    # print('Shape of the samples:', samples_0.shape, samples_1.shape)

    plt.figure(figsize=(4, 4))
    plt.xlim(-M, M)
    plt.ylim(-M, M)
    plt.title(r'Samples from $\pi_0$ and $\pi_1$')
    plt.scatter(samples_0[:, 0].cpu().numpy(), samples_0[:, 1].cpu().numpy(), alpha=0.1, label=r'$\pi_0$')
    plt.scatter(samples_1[:, 0].cpu().numpy(), samples_1[:, 1].cpu().numpy(), alpha=0.1, label=r'$\pi_1$')
    plt.legend()

    plt.tight_layout()
    plt.savefig("distributions.png")
    plt.clf()

    # Training RecFlow 1
    print("Training Recflow 1")
    x_0 = samples_0.detach().clone()[torch.randperm(len(samples_0))]
    x_1 = samples_1.detach().clone()[torch.randperm(len(samples_1))]
    x_pairs = torch.stack([x_0, x_1], dim=1)

    iterations = 10000
    batch_size = 2048
    input_dim = 2

    rectified_flow_1 = RectifiedFlow(model=MLP(input_dim, hidden_num=100), num_steps=100)
    optimizer = torch.optim.Adam(rectified_flow_1.model.parameters(), lr=5e-3)

    rectified_flow_1, loss_curve = train_rectified_flow(rectified_flow_1, optimizer, x_pairs, batch_size, iterations)
    plt.plot(np.linspace(0, iterations, iterations + 1), loss_curve[:(iterations + 1)])
    plt.title('Training Loss Curve')
    plt.savefig("loss_curve_recflow_1.png")

    print("Sampling")
    draw_plot(rectified_flow_1, z0=initial_model.sample([2000]), z1=samples_1.detach().clone(), N=1000)
